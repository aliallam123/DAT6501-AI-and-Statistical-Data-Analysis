{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4edfbe-4324-4d1e-91d4-ee477a5a31c1",
   "metadata": {},
   "source": [
    "#### SPA6330: Artificial Intelligence and Machine Learning\n",
    "#### MO: Dr Linda Cremonesi\n",
    "\n",
    "# Week 3 - Evaluation and Diagnostics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d9371-4dff-4825-aa16-96000bbe10ed",
   "metadata": {},
   "source": [
    "## Research-level datasets\n",
    "\n",
    "This week we start working with a research-level dataset, with more instances and features than in the previous week. The first step should always be **get to know your data**, so try answering these questions:\n",
    "1. **Size:** What was the size of the data set?\n",
    "2. **Missing data:** are there any missing data and how should you deal with them?\n",
    "3. **Outliers:** are there any outliers and how should you deal with them?\n",
    "4. **Balance:** is the data set balanced?\n",
    "5. **Intuition:** which model is it going to work best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fce227-2b65-4a46-91c3-054f86713cd2",
   "metadata": {},
   "source": [
    "### Dealing with missing data\n",
    "\n",
    "There are two main approaches to dealing with missing data in machine learning with scikit-learn:\n",
    "\n",
    "#### 1. Removing Data:\n",
    "\n",
    "This is the simplest option, but it is only suitable if you have a large dataset and losing some data will not significantly impact your analysis.\n",
    "You can drop entire rows or columns containing missing values using methods like `.dropna()` in Pandas.\n",
    "Be cautious, as this approach can introduce bias if the missing data is not random.\n",
    "\n",
    "#### 2. Imputing Missing Values:\n",
    "\n",
    "This involves filling in the missing values with estimated values based on the available data.\n",
    "Scikit-learn offers the `SimpleImputer` class for various imputation strategies:\n",
    "- **Mean/Median Imputation:** Replace missing values with the mean/median of the respective column.\n",
    "- **Most Frequent:** Replace with the most frequent value in the column (for categorical data).\n",
    "- **Constant Value:** Fill with a user-defined constant (be cautious with this).\n",
    "More advanced techniques like kNN imputation or model-based imputation are also available in other libraries.\n",
    "\n",
    "Choose the approach based on your data size, missing value pattern, and acceptable data loss. Evaluate the impact of your chosen method on your model's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c5c407-dfc7-45ea-8e6a-902af45bd4c2",
   "metadata": {},
   "source": [
    "### Dealing with outliers\n",
    "\n",
    "Outliers can create problems to some of our machine learning methods. You can detect outliers with:\n",
    "\n",
    "* **Visualisation:** Tools like boxplots, scatter plots, and heatmaps can help identify potential outliers visually.\n",
    "* **Statistical methods:** Interquartile Range (IQR) or z-scores can indicate data points deviating significantly from the majority.\n",
    "\n",
    "You can then treat them similarly to missing data:\n",
    "* **Removal:** Consider if outliers are errors or truly represent rare cases. Removing can be risky for small datasets.\n",
    "* **Capping:** Replace extreme values with a threshold (e.g., at the IQR boundaries).\n",
    "* **Imputation:** Use strategies like mean/median imputation or more sophisticated methods like KNN imputation.\n",
    "* **Robust models:** Some algorithms are less sensitive to outliers, like Random Forests or Support Vector Machines with appropriate kernels.\n",
    "\n",
    "Always remember that the best approach depends on your data, task, and outlier characteristics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54bd71-995b-4025-9f1b-27b24998b386",
   "metadata": {},
   "source": [
    "## Binary classification evaluation\n",
    "\n",
    "Last week we looked at two performance metrics **accuracy** and the **confusion matrix**. This week we introduce two other concepts **precision** and **recall**. \n",
    "\n",
    "Let's recap what we saw last week first:\n",
    "\n",
    "### Confusion matrix\n",
    "\n",
    "In machine learning, a confusion matrix is a powerful tool for visualising and analysing a classification model's performance. \n",
    "- The matrix is a table with rows and columns representing the **actual classes** and **predicted classes**.\n",
    "- Each cell in the matrix shows the **number of data points** that fall into a specific combination of actual and predicted classes.\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "TN & FP \\\\\n",
    "FN & TP \\\\\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "* **True positives (TP):** Cases where the model correctly predicted a positive class.\n",
    "* **True negatives (TN):** Cases where the model correctly predicted a negative class.\n",
    "* **False positives (FP):** Cases where the model incorrectly predicted a positive class (Type I error).\n",
    "* **False negatives (FN):** Cases where the model incorrectly predicted a negative class (Type II error).\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "In machine learning, **accuracy** is a metric used to measure how often a model correctly predicts the outcome.\n",
    "\n",
    "```\n",
    "Accuracy = (True Negatives + True Positives) / (True Positives + False Positives + False Negatives + True Negatives)\n",
    "```\n",
    "\n",
    "\n",
    "### Precision\n",
    "\n",
    "In machine learning, precision tells you how accurate your positive predictions are. It focuses on **minimising false positives**, which are instances incorrectly classified as belonging to the positive class. \n",
    "\n",
    "```\n",
    "Precision = True Positives / (True Positives + False Positives)\n",
    "```\n",
    "\n",
    "A **high precision** means most of your positive predictions are correct, minimising false positives. This metric may be used in situations where wrongly identifying a positive instance is costly or harmful. Examples include fraud detection or spam filtering.\n",
    "Precision and recall are two commonly used metrics, but they often trade off against each other. Increasing precision may decrease recall (identifying all true positives), and vice versa. Choosing the right metric depends on your specific problem and priorities.\n",
    "\n",
    "\n",
    "### Recall \n",
    "\n",
    "Recall, in the realm of machine learning, serves as a gauge for **completeness**. It tells you how well your model identifies all the actual positive instances, aiming to **minimise false negatives**. It is often crucial in situations where **missing a positive can be costly or harmful**. For example, medical diagnoses. Here's a breakdown:\n",
    "\n",
    "```\n",
    "Recall = True Positives / (True Positives + False Negatives)\n",
    "```\n",
    "\n",
    "A **high recall** means you're catching most of the true positive cases, minimising missed diagnoses.\n",
    "It is often crucial in scenarios where **failing to identify positives can lead to significant consequences**. For example, medical imaging, security threats, etc.\n",
    "Recall and precision are two sides of the coin. While precision focuses on accurate positive predictions, recall emphasises catching all true positives even if it includes some false positives.\n",
    "The choice between prioritising precision or recall depends on your specific context and the potential costs of each type of error.\n",
    "\n",
    "\n",
    "Recall and preicion are mainly used in **classification tasks**, where you predict if an instance belongs to a specific class. For **regression tasks**, other metrics like mean squared error or mean absolute error are more relevant.\n",
    "\n",
    "\n",
    "*For more information on the topic, check out section 5.3 in \"Introduction to Machine Learning with Python\" by MÃ¼ller and Guido.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d573c-9c54-403f-9347-a1f8897858e2",
   "metadata": {},
   "source": [
    "## The ROC curve\n",
    "In machine learning, the **Receiver Operating Characteristic (ROC) curve** is a visual tool that helps evaluate the performance of a **binary classification model**. It plots the **true positive rate (TPR)** against the **false positive rate (FPR)** at various classification thresholds. \n",
    "\n",
    "* **True positive rate (TPR) or Recall:** Proportion of actual positives correctly identified. (Higher is better)\n",
    "* **False positive rate (FPR):** Proportion of actual negatives incorrectly identified as positives. (Lower is better)\n",
    "\n",
    "A perfect model would have an ROC curve that hugs the top left corner, meaning it correctly identifies all true positives (TPR = 1) with no false positives (FPR = 0). A random model would have an ROC curve that follows a diagonal line, indicating no better performance than random guessing. The closer the ROC curve is to the top left corner, the better the model's performance in distinguishing between classes.\n",
    "\n",
    "The area under the ROC curve (**AUC**) is a single numerical value summarising the model's performance, with 1 being perfect and 0.5 being random. ROC curves are particularly useful when the cost of false positives and negatives is different. They can also be used for multi-class classification with some adaptations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d22d45-7e6f-46db-a4fa-9a99e336e5bc",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Cross-validation is a fundamental technique in machine learning used to **evaluate the generalisability** of a model's performance. It essentially involves splitting your data into multiple subsets, training the model on a subset, and testing it on the remaining unseen data. This process is repeated for different folds (subsets) of data, providing a more robust estimate of the model's performance on unseen data compared to simply evaluating it on the entire dataset.\n",
    "\n",
    "`scikit-learn` offers several built-in cross-validation techniques, each with its own advantages and disadvantages:\n",
    "\n",
    "- **K-Fold Cross-Validation:** The data is divided into k equal folds. In each iteration, k-1 folds are used for training, and the remaining fold is used for testing. This process is repeated k times, and the average performance across all folds is calculated.\n",
    "- **Stratified K-Fold Cross-Validation:** Similar to k-fold CV, but ensures each fold preserves the class proportions present in the entire dataset. Useful for imbalanced datasets.\n",
    "- **Leave-One-Out Cross-Validation (LOOCV):** Each data point is used as a test set once, while all other points are used for training. Computationally expensive for large datasets but provides an unbiased estimate.\n",
    "\n",
    "The `cross_val_score` function simplifies cross-validation. You specify your model, data, chosen cross-validation technique, and an evaluation metric (e.g., accuracy, precision, recall). The function then splits the data, trains and tests the model on each fold, and returns an array of performance scores for each fold. You can then calculate the mean and standard deviation of these scores to evaluate the performance. \n",
    "\n",
    "Another option is to use `cross_validate`, which provides a more comprehensive view of your model's performance during cross-validation. It calculates and returns a dictionary containing:\n",
    " - Training and test score per fold for each specified metric.\n",
    " - Fit times and score times for each fold.\n",
    " - Optional: Training data and fitted estimators (if requested).\n",
    "\n",
    "**Key benefits of cross-validation:**\n",
    "\n",
    "- **Prevents overfitting:** Helps identify models that might perform well on the training data but poorly on unseen data.\n",
    "- **Provides a more reliable estimate of model performance:** Offers a broader picture of how the model might generalise to new data.\n",
    "- **Enables comparison of different models:** Allows for fair comparison of different models on the same dataset using the same evaluation strategy.\n",
    "\n",
    "Remember, choosing the appropriate cross-validation technique and metric depends on your specific data and problem. Additionally, tools like learning curves and grid search can be used alongside cross-validation for further model selection and hyperparameter tuning.\n",
    "\n",
    "*For more information on the topic, check out section 5.1 in \"Introduction to Machine Learning with Python\" by MÃ¼ller and Guido.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482d852-0d2f-4981-927a-840ce248b75f",
   "metadata": {},
   "source": [
    "## Diagnostics \n",
    "\n",
    "Overfitting and underfitting are common challenges faced when training machine learning models. They both negatively affect the model's ability to generalise well to unseen data. \n",
    "\n",
    "\n",
    "**Diagnosing with scikit-learn:**\n",
    "\n",
    "* **Cross-validation:** Use techniques like `cross_val_score` or `cross_validate` to evaluate model performance on different subsets of the data.\n",
    "* **Learning curve:** Plot the training and test accuracy/loss as the training data size increases. A large gap between the curves indicates overfitting.\n",
    "* **Model complexity analysis:** Consider the number of parameters and features in your model.\n",
    "\n",
    "\n",
    "**Overfitting:**\n",
    "\n",
    "- Occurs when a model **memorises the training data** too closely, failing to capture the underlying patterns that generalise to new data.\n",
    "- Signs in scikit-learn:\n",
    "    * **High training scores, but low test scores:** The model performs well on the data it trained on but struggles with unseen data.\n",
    "    * **Complex models:** capture noise and irrelevant details in the training data.\n",
    "    * **High variance:** shows that models with many parameters or features are more prone to overfitting.\n",
    "- Tentative solutions:\n",
    "    * **Regularisation:** Use techniques like L1 or L2 regularisation to penalize complex models.\n",
    "    * **Early stopping:** Stop training the model early if validation performance starts to decrease (e.g. limit the number of nodes in a decision tree).\n",
    "    * **Dropout:** Randomly drop neurons during training to prevent overfitting in neural networks.\n",
    "    * **Data augmentation:** Create new training data to increase the diversity of your dataset.\n",
    "    * **Feature selection:** Remove irrelevant or redundant features.\n",
    "\n",
    "\n",
    "**Underfitting:**\n",
    "\n",
    "* Happens when a model is **too simple** and cannot learn the complex relationships within the data.\n",
    "* Signs in scikit-learn:\n",
    "    * **Low scores on both training and test data:** The model fails to learn from the data effectively.\n",
    "    * **Simple models:** unable to capture the underlying patterns in the data.\n",
    "    * **High bias:**, meaning the model consistently misses the true relationship between features and target.\n",
    "* Tentative solutions:\n",
    "    * **More complex models:** Try models with higher capacity, like decision trees with deeper trees or neural networks with more layers.\n",
    "    * **Feature engineering:** Create new features that better capture the relationship between features and target.\n",
    "    * **Ensemble methods:** Use ensemble methods like random forests or gradient boosting, which can combine multiple weaker models to achieve better performance.\n",
    "    * **Collect more training data**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c655e7-5ce2-4b98-838f-22f25447d43d",
   "metadata": {},
   "source": [
    "## Learning curves\n",
    "In machine learning, a **learning curve** is a plot that visualises how a model's performance changes as it learns from more training data. It typically depicts two curves:\n",
    "\n",
    "1. **Training curve:** Shows how the model's performance improves on the data it's used to train on (error or loss decreases, accuracy or other metrics increase).\n",
    "2. **Validation curve:** Shows how the model performs on a separate, unseen validation dataset (ideally representing the real-world data you want to apply it to).\n",
    "\n",
    "By analysing these curves together, you can gain valuable insights into your model's learning behaviour and identify potential issues like **overfitting** and **underfitting**.\n",
    "\n",
    "Learning curves can reveal:\n",
    "\n",
    "* **Overfitting:** If the training curve keeps improving significantly while the validation curve plateaus or even decreases, it suggests the model is memorising the training data too closely and not learning generalisable patterns. This leads to poor performance on unseen data.\n",
    "* **Underfitting:** If both curves remain relatively flat or improve slowly, it indicates the model is too simple and hasn't learned much from the data. This leads to poor performance on both training and unseen data.\n",
    "* **Optimal learning:** A well-behaved learning curve shows the training curve initially improving rapidly, then gradually approaching a plateau while the validation curve follows a similar trend, staying consistently close to the training curve. This indicates the model is learning effectively and generalising well.\n",
    "\n",
    "You can plot learning curves for various metrics depending on your task (e.g., accuracy, precision, recall, loss function). Scikit-learn provides built-in functions like `learning_curve` and `plot_learning_curve` to easily generate and visualise learning curves.\n",
    "\n",
    "By understanding learning curves and interpreting them correctly, you can make informed decisions about your machine learning models, leading to better performance and generalisability on real-world data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
