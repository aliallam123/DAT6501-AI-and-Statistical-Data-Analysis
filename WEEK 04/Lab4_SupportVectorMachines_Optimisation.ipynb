{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a14aa4-1ebb-4769-9658-5d13cba96b13",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bd87637-c26a-4dd3-adb3-c3718a2cb995",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Support Vector Machines (SVMs) are a powerful and versatile set of algorithms in the realm of machine learning. Known for their ability to handle high-dimensional data and resist overfitting, SVMs have become a staple tool for various tasks, including:\n",
    "\n",
    "- **Classification:** Distinguishing between different classes of data, like spam vs. non-spam emails or cat vs. dog images.\n",
    "- **Regression:** Predicting continuous values, such as stock prices or house prices.\n",
    "- **Outlier detection:** Identifying data points that deviate significantly from the norm.\n",
    "\n",
    "**The Core Idea:**\n",
    "\n",
    "Imagine you have data points representing different classes, like red squares and blue circles scattered on a plane. An SVM aims to draw a **hyperplane**, which is like a line in 2D or a plane in higher dimensions, that best separates these classes. However, unlike other algorithms, SVM seeks the **maximum-margin hyperplane**. This means the hyperplane is positioned such that the distance between it and the closest data points from each class (called **support vectors**) is as large as possible. This large margin helps ensure the model generalizes well to new data and avoids overfitting.\n",
    "\n",
    "Let:\n",
    "\n",
    "* $X$ be a set of d-dimensional data points represented as vectors: $X = {x_1, x_2, ..., x_n}$\n",
    "* $y$ be the corresponding class labels: $y = {y_1, y_2, ..., y_n}$, where $y_i ∈ {+1, -1}$ for binary classification\n",
    "\n",
    "\n",
    "A **hyperplane:** in d-dimensions can be represented as:\n",
    "\n",
    "$w^T * x + b = 0$\n",
    "\n",
    "where:\n",
    "\n",
    "* w is a d-dimensional weight vector defining the hyperplane's direction\n",
    "* b is the bias term determining the hyperplane's position\n",
    "\n",
    "The **margin** of a hyperplane is the distance between the closest data points from each class to the hyperplane. It can be calculated as:\n",
    "\n",
    "margin = $2 / ||w||$\n",
    "\n",
    "where $||w||$ is the L2 norm of the weight vector.\n",
    "\n",
    "The SVM seeks to find the hyperplane that maximises the margin while minimising the misclassification error. This translates to a constrained optimisation problem:\n",
    "\n",
    "Minimise:\t$1/2 * ||w||^2$\n",
    "\n",
    "Subject to:\t$y_i * (w^T * x_i + b) ≥ 1$ for all i = 1, 2, ..., n\n",
    "\n",
    "This is a quadratic programming problem that can be solved efficiently using specialised algorithms.\n",
    "\n",
    "In high-dimensional spaces, directly maximising the margin using linear hyperplanes might not be optimal. SVMs leverage **kernels** to implicitly map the data to a higher-dimensional space where a linear separation might be easier. These kernels take two data points as input and calculate their similarity in the transformed space. Commonly used kernels include:\n",
    "\n",
    "* Linear kernel: $ k(x_i, x_j) = x_i^T * x_j$\n",
    "* Polynomial kernel: $k(x_i, x_j) = (x_i^T * x_j + c)^d$\n",
    "* Radial Basis Function (RBF) kernel: $k(x_i, x_j) = exp(-gamma * ||x_i - x_j||^2)$\n",
    "\n",
    "By using kernels, the optimisation problem remains in the original input space, avoiding explicit computation in the high-dimensional space.\n",
    "\n",
    "In real-world data, perfect separation might not be achievable. **Soft margin SVMs** introduce slack variables to allow for some misclassification while still maximising the margin. This introduces a penalty term to the optimisation problem, allowing for a trade-off between margin and misclassification error controlled by a regularisation parameter (C).\n",
    "\n",
    "Solving the constrained optimisation problem directly can be computationally expensive. SVMs often utilise the **dual formulation**, which converts the problem into an equivalent form involving Lagrange multipliers and kernel functions. This leads to a more efficient optimisation process.\n",
    "\n",
    "Data points closest to the hyperplane that influence its position are called **support vectors**. They play a crucial role in defining the decision boundary and are essential for understanding the model's behaviour.\n",
    "\n",
    "**Hyperparameters**\n",
    "Hyperparameters are control knobs that tune the model's behaviour and impact its performance. Choosing the right values for these parameters is crucial for optimal results. Here's a breakdown of key hyperparameters in SVMs:\n",
    "\n",
    "**1. Regularisation parameter (C):**\n",
    "\n",
    "* Controls the trade-off between maximising the margin and minimising misclassification errors.\n",
    "* Higher C values prioritise a wider margin, potentially leading to overfitting on training data and poorer performance on unseen data.\n",
    "* Lower C values allow for more misclassifications but might result in a less robust model with a smaller margin.\n",
    "\n",
    "**2. Gamma (for RBF kernel):**\n",
    "\n",
    "* Controls the influence of individual data points on the decision boundary.\n",
    "* Higher gamma values lead to a more localised decision boundary, fitting the training data closely but potentially overfitting.\n",
    "* Lower gamma values create a smoother decision boundary with less sensitivity to individual data points.\n",
    "\n",
    "**3. Degree parameter (for polynomial kernel):**\n",
    "\n",
    "* Specifies the complexity of the resulting boundary.\n",
    "* A linear kernel corresponds to d=1.\n",
    "* Higher values correspond to increasing model complexity.\n",
    "  \n",
    "**4. Class weights:**\n",
    "\n",
    "* Useful for imbalanced datasets where classes have different sizes.\n",
    "* Assigning higher weights to the minority class penalises misclassifying its instances, encouraging the model to learn better from them.\n",
    "\n",
    "**5. Cost (cost_c):**\n",
    "\n",
    "* This parameter is specific to some SVM implementations and allows assigning different costs to misclassifying different classes.\n",
    "* Useful for problems where misclassifying certain classes has higher consequences.\n",
    "\n",
    "**Choosing Hyperparameters:**\n",
    "\n",
    "This is often done through `GridSearchCV` or `RandomizedSearchCV` in scikit-learn, which try different combinations of values and evaluate their performance using metrics like cross-validation. Visualising the impact of different parameter values on metrics like accuracy, precision, recall, and F1-score can aid in understanding their influence.\n",
    "\n",
    "**Strengths of SVMs:**\n",
    "\n",
    "- **Effective in high dimensions:** Can handle many features without significant performance degradation.\n",
    "- **Robust to overfitting:** The focus on maximising the margin helps prevent the model from overfitting to the training data.\n",
    "- **Interpretability:** The support vectors provide insights into which data points are crucial for the decision boundary.\n",
    "\n",
    "**Challenges of SVMs:**\n",
    "\n",
    "- **Sensitivity to hyperparameters:** Tuning the kernel and C parameters can significantly impact performance.\n",
    "- **Computationally expensive:** Training SVMs can be slower than simpler models for large datasets.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "SVMs have found applications in diverse domains, including:\n",
    "\n",
    "- **Image recognition:** Classifying images of objects or scenes.\n",
    "* **Text classification:** Categorizing emails, news articles, or social media posts.\n",
    "* **Fraud detection:** Identifying fraudulent transactions.\n",
    "* **Bioinformatics:** Analyzing gene expression data to identify disease markers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feeb66a-062d-44a5-befb-e425f3bbe830",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Nested Cross-Validation \n",
    "\n",
    "Nested cross-validation is a powerful technique in machine learning that helps overcome limitations of traditional cross-validation, particularly when tuning hyperparameters. Here's a breakdown of how it works in scikit-learn:\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "Traditional k-fold cross-validation evaluates model performance on unseen data, but it doesn't directly address hyperparameter tuning. Choosing the \"best\" parameters based on cross-validation accuracy can overfit, leading to poor performance on new data.\n",
    "\n",
    "**Nested Cross-Validation to the Rescue:**\n",
    "\n",
    "- **Outer Loop:**\n",
    "    - Splits the dataset into k outer folds.\n",
    "    - For each fold:\n",
    "        - **Inner Loop:**\n",
    "            - Further splits the inner fold into smaller folds (e.g., k' folds).\n",
    "            - For each inner fold:\n",
    "                - Splits the inner fold into training and validation sets.\n",
    "                - Tunes hyperparameters using grid search or random search within the training set.\n",
    "                - Evaluates the tuned model on the validation set.\n",
    "            - Selects the best hyperparameter combination based on average performance across inner folds.\n",
    "        - Trains the final model with the chosen hyperparameters on the entire inner fold (excluding the validation sets).\n",
    "        - Evaluates the final model on the hold-out test set from the outer fold.\n",
    "    - Averages the test set performance across all outer folds to estimate the model's generalizability.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "* **Reduces overfitting:** Hyperparameter tuning and model evaluation happen on separate data, preventing both biases from influencing each other.\n",
    "* **More reliable performance estimation:** Provides a more robust and realistic estimate of how well the model generalises to unseen data.\n",
    "* **Flexible with various algorithms:** Scikit-learn offers tools like `GridSearchCV` and `RandomizedSearchCV` that seamlessly integrate nested cross-validation within various models.\n",
    "\n",
    "**Drawbacks:**\n",
    "\n",
    "* **Computationally expensive:** Requires multiple rounds of training and evaluation, increasing computation time.\n",
    "* **Careful planning needed:** Choosing the right number of folds for both inner and outer loops is crucial to avoid bias and ensure reliable results.\n",
    "\n",
    "Nested cross-validation is a valuable tool for hyperparameter tuning, but it's not a magic bullet. Consider its trade-offs, understand its implementation, and tailor it to your specific problem for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74191c69-8bd6-492c-b33b-0934bea48884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
